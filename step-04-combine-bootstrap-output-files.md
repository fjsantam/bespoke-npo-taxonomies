---
title: "Combining Bootstrap Output Files"
author: "Francisco J. Santamarina"
date: "September 06, 2021"
output:
  html_document:
    keep_md: true
    df_print: paged
    theme: readable
    highlight: tango
    toc: yes
    toc_float: yes
    code_fold: hide
---



In this tutorial document, we will review the fourth step, combining the output files from our bootstrapped classification of nonprofit mission statements. While short, this step is essential for analyzing the classifier's performance data appropriately.

*Additional information on replication steps and data for this project can be found [on this GitHub page](https://fjsantam.github.io/bespoke-npo-taxonomies/)*

# Introduction

Now that we have [run the classifier on our dataset multiple times on training datasets of various sizes](https://fjsantam.github.io/bespoke-npo-taxonomies/step-03-classification-bootstrapping.html), We need to consolidate the various output files into a single dataset containing information on algorithm performance across various dimensions. 

As in the previous steps, we begin by loading the libraries that we will be using.


```{.r .fold-show}
library( dplyr ) # for data wrangling
library( pander ) #attractive tables in rmarkdown
```

## Defining Prediction Classes

Before reading in the data files, we will create a holding object that will preview a mock-up of the results from the .RDS files created in Step 3. If you recall, there are 66 such files - one for each combination of prediction class and cleaned dataset. We have three cleaned datasets, "minimal", "standard", and "custom". We have 22 prediction classes:

* 10 NTEE major groups
* 8 IRS tax exempt purpose codes 
* 4 other variables from the 1023-EZ form commonly used by researchers.

For the duration of this tutorial, we will only focus on the first two categories of prediction classes.

## Preview: Results Data Frame

*Note that all taxonomy classes have been binarized into separate yes/no category codes.* 

The performance metrics that we have identified were generated using the `caret` package in Step 3, specifically the `confusionMatrix()` function. [Section 17.2 of the 'caret` documentation](https://topepo.github.io/caret/measuring-performance.html), "Measures for Predicted Classes," identifies the metrics generated by the function and the formulas used to calculate each.


---------------------------------------------------------------------------
 n.train   n.predict   n.predict.class   prediction.class    metric.type   
--------- ----------- ----------------- ------------------ ----------------
   100        100             7           ntmaj10health      Sensitivity   

   100        100             7           ntmaj10health      Specificity   

   100        100             7           ntmaj10health     Pos.Pred.Value 

   100        100             7           ntmaj10health     Neg.Pred.Value 

   100        100             7           ntmaj10health       Precision    

   100        100             7           ntmaj10health         Recall     
---------------------------------------------------------------------------

Table: Table continues below

 
--------------
 metric.value 
--------------
    0.957     

    0.2857    

    0.9468    

    0.3333    

    0.9468    

    0.957     
--------------

## Data Dictionary

A description of what each term means and how we use them in the code for Steps 4 and 5 follows.

* **n.train** - Number of missions in the training dataset  
* **n.predict** - Number of missions used during predictions made with our trained Naive Bayes model    
* **n.predict.class** - Number of cases in the prediction class out of *n.predict* total cases  
* **prediction.class** - the binarized class that is being predicted 
* **metric.type** - different measures of model accuracy for Naive Bayes models 
* **metric.value** - observed value generated from that sample in the bootstrap 


# Load Results Files

Be sure that the folder with your results (default name "BOOTS-100K.Parallel.Method") is in your working directory. We will set that folder as our new working directory.


```{.r .fold-show}
setwd( "BOOTS-100K.Parallel.Method" )
```

The code below will identify all files in the directory and, based on the file naming convention used in Step 3, will create new datasets based on the type of cleaning approach used to generate the performance data. 


```r
load.these <- dir()

load.standard <- grep( "dat\\.corpus\\.standard", sort(  dir() ), value = TRUE)
load.minimal <- grep( "dat\\.corpus\\.minimal", sort(  dir() ), value = TRUE) 
load.custom <- grep( "dat\\.corpus\\.custom", sort(  dir() ), value = TRUE)

bind_data <- function( file.names )
{
   d <- NULL #Create holding object
   for( j in file.names )
   {
       d.j <- readRDS( j ) #read the the j'th value in the input object and assign it to d.j
       d <- bind_rows( d, d.j ) #bind the rows of data from the j'th value onto the holding object
   }
   return( d ) #Return the holding object
}


df.standard <- bind_data( load.standard ) #Read in all files with "standard" cleaning
df.minimal <- bind_data( load.minimal ) #Read in all files with "minimal" cleaning
df.custom <- bind_data( load.custom ) #Read in all files with "custom" cleaning

#Remove the objects solely consisting of file names from the environment
rm( load.standard ) 
rm( load.minimal )
rm( load.custom )
```

We can preview the first few rows from the results from the "minimal" cleaning set to see how it compares to our mocked up results from earlier.


```r
preview.dat <- df.minimal[1:3,]
row.names(preview.dat) <- NULL
preview.dat %>% pander()
```


----------------------------------------------------------------------------
 n.train   n.predict   n.predict.class   prediction.class     metric.type   
--------- ----------- ----------------- ------------------- ----------------
  4000       20000          1009         Disasterreliefyes    Sensitivity   

  4000       20000          1009         Disasterreliefyes    Specificity   

  4000       20000          1009         Disasterreliefyes   Pos.Pred.Value 
----------------------------------------------------------------------------

Table: Table continues below

 
--------------
 metric.value 
--------------
    0.9899    

   0.04361    

    0.9512    
--------------

# Create consolidated results file

To assist with step 5, Analyze Bootstrap Results, we will save off a new file with the collected results. The new .RDATA file will contain all of the bootstrap results (save off in step 3 as individual .RDS files) parsed into dataframes for "minimal", "standard", and "custom". We will move up a level from our working directory (which we set as the "BOOTS-100K.Parallel.Method" folder earlier ) and create a new, dedicated folder for the consolidated results file.


```{.r .fold-show}
setwd("..") #move up a level
dir.create( "FINAL" )
save( df.standard, df.minimal, df.custom, file = "FINAL/BOOTSTRAP-RESULTS.Rdata" )
```

***

Once this last step is done, we are now ready to analyze our algorithm's classification performance. We will proceed to [Step 5: Analyze Bootstrap Results](https://fjsantam.github.io/bespoke-npo-taxonomies/step-05-analyze-bootstrap-results). 
