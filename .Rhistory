counfound = list( X = "Y[X=1==1" ) )
confounded <- set_confound(
model,
confound = list( X = "Y[X=1==1" ) )
confounded <- set_confound(
model,
confound = list( X = "Y[X=1]==1" ) )
confounded$parameters_df
model_RoUn <- make_model("Modernization -> Unemployment.Rate -> Rise.of.Ultra-nationalism
; Modernization -> Identity.Crisis -> Rise.of.Ultra-nationalism; Migration -> Unemployment.Rate; Migration -> Identity.Crisis")
plot(model_RoUn)
?set_priors
model_RoUn <- make_model("Modernization -> Unemployment.Rate -> Rise.of.Ultra-nationalism
; Modernization -> Identity.Crisis -> Rise.of.Ultra-nationalism; Migration -> Unemployment.Rate; Migration -> Identity.Crisis")
plot(model_RoUn)
#Y = Rise.of.Ultra-nationalism
#M = Identity.Crisis
#Pos. relationship b/w migration and identity crisis
#Restriction: no neg. relationship b/w migration and identity crisis
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "Rise.of.Ultra-nationalism[Migration=0] > Rise.of.Ultra-nationalism[Migration=1]" )
#Y = Rise.of.Ultra-nationalism
#M = Identity.Crisis
#Pos. relationship b/w migration and identity crisis
#Restriction: no neg. relationship b/w migration and identity crisis
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "Rise.of.Ultra-nationalism[Migration=0] > Rise.of.Ultra-nationalism[Migration=1]" )
set_restrictions()
?set_restrictions()
model_RoUn <- make_model("Modernization -> Unemployment.Rate -> RoUn
; Modernization -> Identity.Crisis -> RoUn; Migration -> Unemployment.Rate; Migration -> Identity.Crisis")
plot(model_RoUn)
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[Migration=0] > RoUn[Migration=1]" )
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "Identity.Crisis[Migration=0] > Identity.Crisis[Migration=1]" )
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[Migration=0] > RoUn[Migration=1]" ) #has to be with outcome node
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[Identity.Crisis=0] > RoUn[Identity.Crisis=1]" ) #has to be with outcome node
model_RoUn <- make_model("Modernization -> Unemployment.Rate -> RoUn
; Modernization -> Identity.Crisis -> RoUn; Migration -> Unemployment.Rate; Migration -> Identity.Crisis")
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[Identity.Crisis=0] > RoUn[Identity.Crisis=1]" ) #has to be with outcome node
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[Identity.Crisis=0]>RoUn[Identity.Crisis=1]" ) #has to be with outcome node
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn['Identity.Crisis'=0]>RoUn['Identity.Crisis'=1]" ) #has to be with outcome node
?set_restrictions
model_RoUn <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis")
plot(model_RoUn)
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" ) #has to be with outcome node
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = c("RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" )) #has to be with outcome node
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[IdentityCrisis=1]>RoUn[IdentityCrisis=0]" ) #has to be with outcome node
# Restriction: no neg. relationship b/w RoUn and identity crisis
model_RoUn <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis")
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" ) #has to be with outcome node
# The restriction below
# removes the nodal type Y.10
modelXMY <- make_model(
"X -> M -> Y"
)
restricted_model <- set_restrictions(
model = modelXMY,
statement =
"Y[M=0]>Y[M=1]"
)
model_RoUn_r <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis") %>%
set_restrictions( statement = "RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" ) #has to be with outcome
model_RoUn <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis")
model_RoUn_r <- set_restrictions(
model = model_RoUn,
statement = "RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" ) #has to be with outcome node
model_RoUn_r <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis") %>%
set_restrictions( statement = "RoUn[IdentityCrisis=0]>RoUn[IdentityCrisis=1]" ) #has to be with outcome
plot( model_RoUn_r )
modelXMY <- make_model(
"X -> M -> Y"
)
restricted_model <- set_restrictions(
model = modelXMY,
statement =
"Y[M=0]>Y[M=1]"
)
plot( modelXMY)
plot(restricted_model)
model_RoUn_r_p <- set_priors(
model = restricted_model_RoUn,
# set a value of 15 for the alpha that captures the uncertainty
# about the share of Y.10
# set a value of 0.5 for the alpha that captures the uncertainty
# about the share of Y.01
#statement = c("Y[M=0]>Y[M=1]",
#              "Y[M=1]>Y[M=0]"),
statement = c(
"Rise.of.Ultra-nationalism[Unemployment.Rate=0]>Rise.of.Ultra-nationalism[Unemployment.Rate=1]",
"Rise.of.Ultra-nationalism[Unemployment.Rate=1]>Rise.of.Ultra-nationalism[Unemployment.Rate=0]"),
alphas = c(15, 0.5)
)
model_RoUn_r_p <- set_priors(
model = model_RoUn_r,
# set a value of 15 for the alpha that captures the uncertainty
# about the share of Y.10
# set a value of 0.5 for the alpha that captures the uncertainty
# about the share of Y.01
#statement = c("Y[M=0]>Y[M=1]",
#              "Y[M=1]>Y[M=0]"),
statement = c(
"Rise.of.Ultra-nationalism[Unemployment.Rate=0]>Rise.of.Ultra-nationalism[Unemployment.Rate=1]",
"Rise.of.Ultra-nationalism[Unemployment.Rate=1]>Rise.of.Ultra-nationalism[Unemployment.Rate=0]"),
alphas = c(15, 0.5)
)
model_RoUn_r_p <- set_priors(
model = model_RoUn_r,
# set a value of 15 for the alpha that captures the uncertainty
# about the share of Y.10
# set a value of 0.5 for the alpha that captures the uncertainty
# about the share of Y.01
#statement = c("Y[M=0]>Y[M=1]",
#              "Y[M=1]>Y[M=0]"),
statement = c(
"RoUn[UnemploymentRate=0]>RoUn[UnemploymentRate=1]",
"RoUn[UnemploymentRate=1]>RoUn[UnemploymentRate=0]"),
alphas = c(15, 0.5)
)
model_RoUn_r_p_c <- set_confound(
model = model_RoUn_r_p,
confound = "RoUn <-> UnemploymentRate")
#model_RoUn_r_p_c <- set_confound(
#model = model_RoUn_r_p,
model_RoUn_c <- set_confound(
model = model_RoUn,
confound = "RoUn <-> UnemploymentRate")
model_RoUn <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis")
plot(model_RoUn)
model_RoUn <- make_model("Modernization -> UnemploymentRate -> RoUn
; Modernization -> IdentityCrisis -> RoUn; Migration -> UnemploymentRate; Migration -> IdentityCrisis")
#model_RoUn_r_p_c <- set_confound(
#model = model_RoUn_r_p,
model_RoUn_c <- set_confound(
model = model_RoUn,
confound = "RoUn <-> UnemploymentRate")
install.packages("gender")
install.packages(c("broom", "colorspace", "curl", "gert", "ggplot2", "glmmTMB", "glmnet", "lme4", "mime", "spatstat", "spatstat.linnet"))
install.packages(c("bayestestR", "blob", "broom", "broom.mixed", "car", "cli", "corrplot", "cpp11", "credentials", "e1071", "emmeans", "fastmatch", "gargle", "ggeffects", "glmmTMB", "googledrive", "googlesheets4", "haven", "isoband", "jpeg", "lavaan", "lwgeom", "matrixStats", "maxLik", "mgsub", "parallelly", "performance", "pillar", "protolite", "rbibutils", "Rcpp", "RcppArmadillo", "rdrobust", "readr", "RPostgreSQL", "Rttf2pt1", "rvest", "sf", "sjPlot", "spatstat.core", "spatstat.geom", "spatstat.linnet", "stringdist", "stringi", "survey", "tau", "testthat", "tibble", "utf8", "wk"))
install.packages("sf")
load("C:/Users/franc/Dropbox/Education/Writing/.Dissertation/Drafts/02_Paper 1_Rat_Prof_Standards/Impact Eval STM Demo 210811.RData")
terms( dat_lda[[1]], n = 12 )
library( seededlda )
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, fig.width=10)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, fig.width=10)
# Chunk 2
library( dplyr )
library( knitr )
d <- read.csv( "data/model-accuracy.csv" )
d$f <- d$prediction.class
d$f[ d$f == "Disasterreliefyes"  ] <- "Disaster Relief"
d$f[ d$f == "Donatefundsyes"  ] <- "Donative"
d$f[ d$f == "ntmaj10art"  ] <- "NTMAJ Art"
d$f[ d$f == "ntmaj10edu"  ] <- "NTMAJ Education"
d$f[ d$f == "ntmaj10env"  ] <- "NTMAJ Environment"
d$f[ d$f == "ntmaj10health"  ] <- "NTMAJ Health"
d$f[ d$f == "ntmaj10hserv"  ] <- "NTMAJ Human Service"
d$f[ d$f == "ntmaj10int"  ] <- "NTMAJ International"
d$f[ d$f == "ntmaj10mutual"  ] <- "NTMAJ Mutual Benefit"
d$f[ d$f == "ntmaj10public"  ] <- "NTMAJ Public"
d$f[ d$f == "ntmaj10rel"  ] <- "NTMAJ Religion"
d$f[ d$f == "ntmaj10unknown"  ] <- "NTMAJ Unknown"
d$f[ d$f == "Onethirdsupportgifts"  ] <- "Support by Gift"
d$f[ d$f == "Onethirdsupportpublic"   ] <- "Support by Public"
d$f[ d$f == "Orgpurposecharitable"  ] <- "Charitable Purpose"
d$f[ d$f == "Orgpurposecrueltyprevention"  ] <- "Cruelty Prevention"
d$f[ d$f == "Orgpurposeeducational"   ] <- "Education"
d$f[ d$f == "Orgpurposeliterary"  ] <- "Literary"
d$f[ d$f == "Orgpurposepublicsafety"  ] <- "Public Safety"
d$f[ d$f == "Orgpurposereligious"  ] <- "Religious"
d$f[ d$f == "Orgpurposescientific"  ] <- "Scientific"
d$f[ d$f == "Orgpurposeamateursports"  ] <- "Amateur Sports"
d <- arrange( d, dataset, f, n.train )
head( d, 21 )
d$dy <- 100* ( c( d$value[-1], NA ) - d$value )
# 100 * ( new - old), where
## old = current training dataset size, ex. 4,000
## new = next training dataset size up, ex. 8,000 is one size up from 4,000
d$dy[ d$n.train == 4000 ] <- NA #can't go from 0 to 4,000
d$dy[ d$n.train == 80000 ] <- NA #can't go from 80,000 to 84,000
d %>%
group_by( n.train ) %>%
summarize( bal_accuracy=100*mean(value), ave=mean(dy) ) #renamed accuracy to bal_accuracy
library( dplyr )
library( knitr )
d <- read.csv( "data/model-accuracy.csv" )
d$f <- d$prediction.class
d$f[ d$f == "Disasterreliefyes"  ] <- "Disaster Relief"
d$f[ d$f == "Donatefundsyes"  ] <- "Donative"
d$f[ d$f == "ntmaj10art"  ] <- "NTMAJ Art"
d$f[ d$f == "ntmaj10edu"  ] <- "NTMAJ Education"
d$f[ d$f == "ntmaj10env"  ] <- "NTMAJ Environment"
d$f[ d$f == "ntmaj10health"  ] <- "NTMAJ Health"
d$f[ d$f == "ntmaj10hserv"  ] <- "NTMAJ Human Service"
d$f[ d$f == "ntmaj10int"  ] <- "NTMAJ International"
d$f[ d$f == "ntmaj10mutual"  ] <- "NTMAJ Mutual Benefit"
d$f[ d$f == "ntmaj10public"  ] <- "NTMAJ Public"
d$f[ d$f == "ntmaj10rel"  ] <- "NTMAJ Religion"
d$f[ d$f == "ntmaj10unknown"  ] <- "NTMAJ Unknown"
d$f[ d$f == "Onethirdsupportgifts"  ] <- "Support by Gift"
d$f[ d$f == "Onethirdsupportpublic"   ] <- "Support by Public"
d$f[ d$f == "Orgpurposecharitable"  ] <- "Charitable Purpose"
d$f[ d$f == "Orgpurposecrueltyprevention"  ] <- "Cruelty Prevention"
d$f[ d$f == "Orgpurposeeducational"   ] <- "Education"
d$f[ d$f == "Orgpurposeliterary"  ] <- "Literary"
d$f[ d$f == "Orgpurposepublicsafety"  ] <- "Public Safety"
d$f[ d$f == "Orgpurposereligious"  ] <- "Religious"
d$f[ d$f == "Orgpurposescientific"  ] <- "Scientific"
d$f[ d$f == "Orgpurposeamateursports"  ] <- "Amateur Sports"
d <- arrange( d, dataset, f, n.train )
head( d, 21 )
d$dy <- 100* ( c( d$value[-1], NA ) - d$value )
# 100 * ( new - old), where
## old = current training dataset size, ex. 4,000
## new = next training dataset size up, ex. 8,000 is one size up from 4,000
d$dy[ d$n.train == 4000 ] <- NA #can't go from 0 to 4,000
d$dy[ d$n.train == 80000 ] <- NA #can't go from 80,000 to 84,000
d %>%
group_by( n.train ) %>%
summarize( bal_accuracy=100*mean(value), ave=mean(dy) ) #renamed accuracy to bal_accuracy
getwd()
setwd("C:/Users/franc/Dropbox/Research_Shared Folders/USC Mission Paper/Drafts/bespoke-taxonomies/FRANCISCO")
library( dplyr )
library( knitr )
d <- read.csv( "data/model-accuracy.csv" )
d$f <- d$prediction.class
d$f[ d$f == "Disasterreliefyes"  ] <- "Disaster Relief"
d$f[ d$f == "Donatefundsyes"  ] <- "Donative"
d$f[ d$f == "ntmaj10art"  ] <- "NTMAJ Art"
d$f[ d$f == "ntmaj10edu"  ] <- "NTMAJ Education"
d$f[ d$f == "ntmaj10env"  ] <- "NTMAJ Environment"
d$f[ d$f == "ntmaj10health"  ] <- "NTMAJ Health"
d$f[ d$f == "ntmaj10hserv"  ] <- "NTMAJ Human Service"
d$f[ d$f == "ntmaj10int"  ] <- "NTMAJ International"
d$f[ d$f == "ntmaj10mutual"  ] <- "NTMAJ Mutual Benefit"
d$f[ d$f == "ntmaj10public"  ] <- "NTMAJ Public"
d$f[ d$f == "ntmaj10rel"  ] <- "NTMAJ Religion"
d$f[ d$f == "ntmaj10unknown"  ] <- "NTMAJ Unknown"
d$f[ d$f == "Onethirdsupportgifts"  ] <- "Support by Gift"
d$f[ d$f == "Onethirdsupportpublic"   ] <- "Support by Public"
d$f[ d$f == "Orgpurposecharitable"  ] <- "Charitable Purpose"
d$f[ d$f == "Orgpurposecrueltyprevention"  ] <- "Cruelty Prevention"
d$f[ d$f == "Orgpurposeeducational"   ] <- "Education"
d$f[ d$f == "Orgpurposeliterary"  ] <- "Literary"
d$f[ d$f == "Orgpurposepublicsafety"  ] <- "Public Safety"
d$f[ d$f == "Orgpurposereligious"  ] <- "Religious"
d$f[ d$f == "Orgpurposescientific"  ] <- "Scientific"
d$f[ d$f == "Orgpurposeamateursports"  ] <- "Amateur Sports"
d <- arrange( d, dataset, f, n.train )
head( d, 21 )
d$dy <- 100* ( c( d$value[-1], NA ) - d$value )
# 100 * ( new - old), where
## old = current training dataset size, ex. 4,000
## new = next training dataset size up, ex. 8,000 is one size up from 4,000
d$dy[ d$n.train == 4000 ] <- NA #can't go from 0 to 4,000
d$dy[ d$n.train == 80000 ] <- NA #can't go from 80,000 to 84,000
d %>%
group_by( n.train ) %>%
summarize( bal_accuracy=100*mean(value), ave=mean(dy) ) #renamed accuracy to bal_accuracy
graph.df <-
d %>%
group_by( dataset, f ) %>%
summarize( threshold.50=n.train[ which.max( dy < 0.5 )  -1 ],
threshold.25=n.train[ which.max( dy < 0.25 ) -1 ],
performance.50=value[ which.max( dy < 0.5 ) -1 ],
performance.25=value[ which.max( dy < 0.25 ) -1 ] ) %>%
group_by( f ) %>%
mutate( ave.p.50=mean(performance.50),
ave.p.25=mean(performance.25) ) %>%
ungroup() %>%
arrange( - ave.p.50 ) %>%
as.data.frame()
head( graph.df, 20 )
df <- graph.df
cex.factor <- 1.5
min.x <- min( df$threshold.50 )
max.x <- max( df$threshold.50 )
df2 <- filter( df, ave.p.50 > 0.60 )
df3 <- filter( df, ave.p.50 < 0.60 )
mean.minimal.2 <- mean( df2$threshold.50[ df2$dataset=="Minimal" ] )
mean.standard.2 <- mean( df2$threshold.50[ df2$dataset=="Standard" ] )
mean.custom.2 <- mean( df2$threshold.50[ df2$dataset=="Custom" ] )
mean.minimal.3 <- mean( df3$threshold.50[ df3$dataset=="Minimal" ] )
mean.standard.3 <- mean( df3$threshold.50[ df3$dataset=="Standard" ] )
mean.custom.3 <- mean( df3$threshold.50[ df3$dataset=="Custom" ] )
df$threshold.50[ df$dataset == "Minimal" ] <-
df$threshold.50[ df$dataset == "Minimal" ] - 1000
df$threshold.50[ df$dataset == "Custom" ] <-
df$threshold.50[ df$dataset == "Custom" ] + 1000
df.ave.p <- unique(df[c("f","ave.p.50")])
p.class <- df.ave.p$f
mean.p <- df.ave.p$ave.p.50
mean.p <- as.character( mean.p %>% round(4) )
mean.p <- substr( mean.p, 1, 4 )
#col.group <- adjustcolor( c("steelblue","firebrick","darkgreen"), alpha.f=0.5 )
col.group <- adjustcolor( c("deepskyblue3","firebrick3","green3"), alpha.f=0.5 ) #standardizing colors with box and whisker plot
par( mar=c(5,12,4,4) ) #tweaked left margin to not cut off names
plot.new()
plot.window( xlim=c(min.x-2000,max.x+2000), ylim=c(1,24) )
# abline( v=seq(min.x,max.x,4000), h=1:10, lty=3, col="gray70" )
abline( h=22:10, lty=3, col="gray70" )
y.pos <- 22
# mean.p <- NULL
for( i in p.class )
{
dd <- filter( df, f == i )
if( y.pos > 9 )
{
points( dd$threshold.50, rep(y.pos,3),
col=col.group, pch=19,
cex=2*cex.factor )
}
if( y.pos <= 9 )
{
points( dd$threshold.50, rep(y.pos,3),
col=gray(0.5,0.5), pch=19,
cex=2*cex.factor )
}
y.pos <- y.pos - 1
}
axis( side=2, at=22:10, labels=p.class[1:13],
las=2, tick=F, cex.axis=0.8*cex.factor )
axis( side=2, at=9:1, labels=p.class[14:22],
las=2, tick=F, cex.axis=0.8*cex.factor,
col.axis="gray40" )
axis( side=1, at=seq(min.x,max.x,4000),
labels=paste0( seq(min.x/1000,max.x/1000,4), "k" ),
las=1, tick=F, cex.axis=0.7*cex.factor )
axis( side=4, at=22:1, las=2, tick=F,
labels=mean.p, col.axis="gray40", cex.axis=0.7*cex.factor )
title( xlab="Number of Training Cases", cex.lab=cex.factor )
# mean.minimal <- median( df$threshold.50[ df$dataset=="Minimal" ] )
# mean.standard <- median( df$threshold.50[ df$dataset=="Standard" ] )
# mean.custom <- median( df$threshold.50[ df$dataset=="Custom" ] )
#
# abline( v=mean.minimal, lwd=3, col=adjustcolor("firebrick", alpha.f=0.5) )
# abline( v=mean.standard, lwd=3, col=adjustcolor("darkgreen", alpha.f=0.5) )
# abline( v=mean.custom, lwd=3, col=adjustcolor("steelblue", alpha.f=0.5) )
segments( x0=mean.minimal.2, y0=10, y1=22,
lwd=3*cex.factor, col=adjustcolor("green3", alpha.f=0.5) )
segments( x0=mean.standard.2, y0=10, y1=22,
lwd=3*cex.factor, col=adjustcolor("firebrick3", alpha.f=0.5) )
segments( x0=mean.custom.2, y0=10, y1=22,
lwd=3*cex.factor, col=adjustcolor("deepskyblue3", alpha.f=0.5) )
text( 17000, 10, "~17k", pos=1, col="gray10", cex=1.2*cex.factor )
segments( x0=mean.minimal.3, y0=1, y1=9,
lwd=3*cex.factor, col=gray(0.5,0.5) )
segments( x0=mean.standard.3, y0=1, y1=9,
lwd=3*cex.factor, col=gray(0.5,0.5) )
segments( x0=mean.custom.3, y0=1, y1=9,
lwd=3*cex.factor, col=gray(0.5,0.5) )
title( main="Optimal Training Dataset Size", cex.main=cex.factor )
yy <- 24
xx <- 2000
text( xx-2000, yy, "DATA PREP:", pos=4,
cex=0.80*cex.factor, col="gray20" )
text( xx+12000, yy, "BASIC", col="green3",
cex=0.70*cex.factor, pos=4 )
points( xx+12000, yy, pch=19,
cex=0.9*cex.factor, col="green3" )
text( xx+20000, yy, "STANDARD", col="firebrick3",
cex=0.70*cex.factor, pos=4 )
points( xx+20000, yy, pch=19,
cex=0.9*cex.factor, col="firebrick3" )
text( xx+30000, yy, "CUSTOM",
col="deepskyblue3", cex=0.70*cex.factor, pos=4 )
points( xx+30000, yy, pch=19,
cex=0.9*cex.factor, col="deepskyblue3" )
text( 28000, 4, "HIDDEN \nCLASSES", col="gray80", cex=1.3*cex.factor )
text( 40350, 22.5, "Balanced Accuracy:", col="gray40", pos=2)
#Add vertical lines for the shown classes only
# abline( v=seq(min.x,max.x,4000), h=1:10, lty=3, col="gray70" )
mult_seg <- data.frame(x0 = seq(min.x,max.x,4000),    # Create data frame with line-values
y0 = 10,
y1 = 22)
segments(x0 = mult_seg$x0,                            # Draw multiple lines
y0 = mult_seg$y0,
y1 = mult_seg$y1,
col = "gray70",
lty = 3)
getwd()
# Chunk 1: setup
knitr::opts_chunk$set( message=F, warning=F, fig.width=10 )
# Set the local address - not necessary for this step
#knitr::opts_knit$set( progress=T, root.dir= "data/step-05-files-to-analyze")
# Chunk 2
library( dplyr ) # for data wrangling
library( DT ) # for datatables and interactive tables in rmarkdown
library( ggplot2 ) # for visualizations
library( scales ) # for visualizations
# Chunk 3
load( "data/step-05-files-to-analyze/BOOTSTRAP-RESULTS.Rdata" )
# Chunk 5
df.types <- c("Minimal", "Standard", "Custom")
df <- cbind( df.minimal, df.types[1] ) #bind a column for dataset to each row
colnames( df)[7] <- "dataset" #rename the dataset
df.holding <- cbind( df.standard, df.types[2] ) #repeat the earlier step with a temporary object
colnames( df.holding )[7] <- "dataset"
df <- rbind( df, df.holding ) #bind the temporary object onto the dataframe
df.holding <- cbind( df.custom, df.types[3] )
colnames( df.holding )[7] <- "dataset"
df <- rbind( df, df.holding )
rm(df.holding)
# Chunk 6
str( df ) #preview the structure
# Chunk 7
dim.old <- comma( nrow( df ) ) #number of rows before removing NAs
dim.NA <- comma(
length( df$metric.value[
is.na( df$metric.value ) |
is.nan( df$metric.value ) ]
)
) #number of rows with missing values
# Replace either category of missing values with "NA"
df$metric.value[ is.na( df$metric.value ) | is.nan( df$metric.value ) ] <- NA
#Remove rows with missing values from the dataset
df <- na.omit( df )
dim.new <- comma( nrow( df ) )
naOutput <- data.frame(
rbind( dim.old, dim.NA, dim.new ),
row.names = c("Original number of rows", "Rows with missing values (NA or NaN)", "Rows without missing values")
)
colnames( naOutput ) <- ""
print( "Comparison of the original count of rows, rows with NAs, and  new count of rows " )
naOutput
# Chunk 8
head( df.minimal[ is.na(df.minimal$metric.value), ], 4 )
# Chunk 9
# Relevel metric type factor - convert from characters to factors
df$metric.type <- factor( df$metric.type )
# Find average of each metric by training data size
df.summary <-
df %>%
group_by( n.train, prediction.class, metric.type, dataset  ) %>%
summarize( value = mean( metric.value ),
min=min(metric.value),
max=max(metric.value),
sd=sd(metric.value),
class.prop=mean(n.predict.class/100) )
levels( df.summary$metric.type )
# Chunk 10
dim(df.summary)
# Chunk 11
head( as.data.frame(df.summary), 9 )
# Chunk 14
df.demo <-
df.summary %>%
filter( prediction.class=="ntmaj10art" & metric.type=="Balanced.Accuracy" & dataset=="Standard")
head( as.data.frame(df.demo), 6 )
# Chunk 15
x <- df.demo$n.train
y <- df.demo$value
plot( x, y, type="b",  bty="n", xaxt="n", yaxt="n",
pch=19, col=gray( 0.0,0.25), cex=1.8,
xlab="Training Dataset Size", ylab="Balanced Accuracy",
main = "Preview of Averaged Balanced Accuracy for NTEE\nMajor 10 Group Art with Standard Cleaning",
cex.main = 1)
axis( side = 2, at = seq(0.01, 1.00, 0.01) )
axis( side = 1, at= seq(0,max(df.demo$n.train),max(df.demo$n.train)/10) )
abline( h=seq(0.01, 1.00, 0.01), lwd=0.5, col=gray(0.7,0.3) )
abline( v=seq(0,max(df.demo$n.train),max(df.demo$n.train)/10), lwd=0.5, col=gray(0.7,0.3) )
lines( lowess(x,y), col="firebrick", lty=1, lwd=4 )
box( col=gray(0.7,0.3) )
points( x, y, type="b", pch=19, col=gray( 0.0,0.25), cex=1.8 )
# Chunk 16
classes <- unique( df$prediction.class )
classes.1 <- classes[ c(1:2, 13, 14) ]
classes.nt <- classes[3:12]
classes.te <- classes[15:22]
metrics <- c("Accuracy", "Balanced.Accuracy", "Detection.Prevalence",
"Detection.Rate", "F1", "Kappa", "Neg.Pred.Value",
"Pos.Pred.Value", "Precision", "Prevalence",
"Sensitivity", "Specificity")
# Chunk 17
df.summary <-
df.summary %>%
filter( metric.type=="Balanced.Accuracy" )
head( as.data.frame(df.demo), 6 )
# Chunk 18
dim(df.summary)
sort(unique(df.summary$prediction.class))
